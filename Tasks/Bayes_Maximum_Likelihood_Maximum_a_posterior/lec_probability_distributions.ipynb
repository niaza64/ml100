{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability distributions\n",
    "\n",
    "At the heart of the previously covered probabilistic methods and, in particular, Bayesian classification, are the prior probabilities $P(A)$ and the conditional probabilities $P(x|A)$ (or for probability density $p(x|A)$). It is important to understand the main tools and therefore we will study Gaussian distribution with all flavors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting distribution\n",
    "\n",
    "We will focus on Gaussian for the sake of time, but similar treatment should be given to all of them:\n",
    "\n",
    " * https://en.wikipedia.org/wiki/List_of_probability_distributions\n",
    " \n",
    " In addition to the Gaussian the following might be familiar to you:\n",
    " \n",
    "  * Bernoulli distribution - modeling the coin toss\n",
    "  * Poisson distribution - modeling arrival of photons to mobile camera sensor or phone calls to call center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "#\n",
    "# 1. Bernoulli parameter estimation (ML)\n",
    "np.random.seed(666) # to always get the same points\n",
    "\n",
    "mu = 0.3\n",
    "N_seq = np.array([1,2,4,6,8,10,12,14,16,18,20,25,30,35,40,45,50])\n",
    "mu_ML = np.empty(N_seq.shape)\n",
    "a=4\n",
    "b=6\n",
    "a2=40\n",
    "b2=60\n",
    "mu_MAP = np.empty(N_seq.shape)\n",
    "mu_MAP2 = np.empty(N_seq.shape)\n",
    "for N_num, N in enumerate(N_seq):\n",
    "    x_n = np.random.binomial(1,mu,size=N)\n",
    "    mu_ML[N_num] = sum(x_n)/N\n",
    "    mu_MAP[N_num] = (sum(x_n)+a)/(N+a+b)\n",
    "    mu_MAP2[N_num] = (sum(x_n)+a2)/(N+a2+b2)\n",
    "\n",
    "#\n",
    "# Plot 1 - only ML\n",
    "fig = plt.figure(figsize =(8, 4))\n",
    "plt.plot(N_seq, mu_ML,'--', label='mu ML')\n",
    "plt.plot([0,N],[mu,mu],'-')\n",
    "plt.xticks(N_seq)\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('mu')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "# Plot 2 - ML  plus 2x MAP\n",
    "fig = plt.figure(figsize =(8, 4))\n",
    "plt.plot([0,N],[mu,mu],label='mu')\n",
    "plt.plot(N_seq, mu_ML,label='mu ML')\n",
    "plt.plot(N_seq, mu_MAP,label='mu MAP (a=4,b=6)')\n",
    "plt.plot(N_seq, mu_MAP2,label='mu MAP (a=40,b=60)')\n",
    "plt.xticks(N_seq)\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('mu')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian distribution for one observation variable\n",
    "\n",
    "The most common distribution used in course book experiments is the Gaussian distribution which is also known as the *normal distribution*  $\\mathcal{N}$.\n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathcal{N}(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\enspace ,\n",
    "  \\label{eq:gaussian}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's study the meaning of the two parameters $\\mu$ and $\\sigma^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's plot a few different gaussians\n",
    "np.random.seed(666) # to always get the same points\n",
    "\n",
    "x = np.linspace(-6.0,6.0,101)\n",
    "mu_1 = 0.0\n",
    "sigma2_1 = 2\n",
    "gauss_1 = 1/np.sqrt(2*np.pi*sigma2_1)*np.exp(-1/(2*sigma2_1)*(x-mu_1)**2)\n",
    "mu_2 = 2.0\n",
    "sigma2_2 = 2\n",
    "gauss_2 = 1/np.sqrt(2*np.pi*sigma2_2)*np.exp(-1/(2*sigma2_2)*(x-mu_2)**2)\n",
    "mu_3 = 2.0\n",
    "sigma2_3 = 4\n",
    "gauss_3 = 1/np.sqrt(2*np.pi*sigma2_3)*np.exp(-1/(2*sigma2_3)*(x-mu_3)**2)\n",
    "\n",
    "\n",
    "#\n",
    "# Plot 1\n",
    "plt.plot(x, gauss_1,'c-',label='mu=0.0, sigma2=2.0')\n",
    "plt.plot(x, gauss_2,'m-',label='mu=2.0, sigma2=2.0')\n",
    "plt.plot(x, gauss_3,'y-',label='mu=2.0, sigma2=4.0')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now study estimation of the two parameters $\\mu$ and $\\sigma^2$.\n",
    "\n",
    "Not also the meaning of [percentile](https://en.wikipedia.org/wiki/Percentile) - why it make sense to include standard deviation when you report errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate N samples using the previously defined paraters (test N=2,10,100)\n",
    "N_est = 2\n",
    "\n",
    "x_gauss_1 = np.random.normal(mu_1,np.sqrt(sigma2_1),N_est)\n",
    "x_gauss_2 = np.random.normal(mu_2,np.sqrt(sigma2_2),N_est)\n",
    "x_gauss_3 = np.random.normal(mu_2,np.sqrt(sigma2_3),N_est)\n",
    "mu_gauss_1 = np.mean(x_gauss_1)\n",
    "mu_gauss_2 = np.mean(x_gauss_2)\n",
    "mu_gauss_3 = np.mean(x_gauss_3)\n",
    "var_gauss_1 = np.std(x_gauss_1)**2\n",
    "var_gauss_2 = np.std(x_gauss_2)**2\n",
    "var_gauss_3 = np.std(x_gauss_3)**2\n",
    "\n",
    "est_gauss_1 = 1/np.sqrt(2*np.pi*var_gauss_1)*np.exp(-1/(2*var_gauss_1)*(x-mu_gauss_1)**2)\n",
    "est_gauss_2 = 1/np.sqrt(2*np.pi*var_gauss_2)*np.exp(-1/(2*var_gauss_2)*(x-mu_gauss_2)**2)\n",
    "est_gauss_3 = 1/np.sqrt(2*np.pi*var_gauss_3)*np.exp(-1/(2*var_gauss_3)*(x-mu_gauss_3)**2)\n",
    "\n",
    "#\n",
    "# Plot 2 (w/ estimated parameters)\n",
    "plt.plot(x, gauss_1,'c-',label='mu=0.0, sigma2=2.0')\n",
    "plt.plot(x, est_gauss_1,'c--',label='estimated')\n",
    "plt.plot(x, gauss_2,'m-',label='mu=2.0, sigma2=2.0')\n",
    "plt.plot(x, est_gauss_2,'m--',label='estimated')\n",
    "plt.plot(x, gauss_3,'y-',label='mu=2.0, sigma2=4.0')\n",
    "plt.plot(x, est_gauss_3,'y--',label='estimated')\n",
    "plt.title(f'Parameter estimation using {N_est} samples')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two observation variables - 2D Gaussian\n",
    "\n",
    "Let's assume we have two dimensional measurements $\\vec{x} = (x_1,x_2)^T$, for example $x_1$ is height and $x_2$ is weight. Let's plot these measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's make 2D correlated data of hobbits and elfs\n",
    "\n",
    "# Hobits\n",
    "np.random.seed(42) # to always get the same points\n",
    "N_h = 80 \n",
    "x1_h = np.random.normal(1.1,0.3,N_h)\n",
    "a_h = 50.0\n",
    "b_h = 20.0\n",
    "x2_h_noise =  np.random.normal(0,8,N_h)\n",
    "x2_h = a_h*x1_h+b_h+x2_h_noise\n",
    "\n",
    "# Elves\n",
    "N_e = 20 \n",
    "x1_e = np.random.normal(1.9,0.4,N_e)\n",
    "a_e = 30.0\n",
    "b_e = 30.0\n",
    "x2_e_noise =  np.random.normal(0,8,N_e)\n",
    "x2_e = a_e*x1_e+b_e+x2_e_noise\n",
    "\n",
    "#\n",
    "# PLOT 1 both classes in 1D\n",
    "plt.plot(x1_h,np.zeros([N_h,1]),'co', label=\"hobbit\")\n",
    "plt.plot(x1_e,np.zeros([N_e,1]),'mo', label=\"elf\")\n",
    "plt.title('Training samples from two classes c1 and c2')\n",
    "plt.legend()\n",
    "plt.xlabel('height [m]')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(x1_h, bins = 10, alpha=0.5, color='cyan')\n",
    "plt.hist(x1_e, bins = 10, alpha=0.5, color='magenta')\n",
    "plt.title('Training samples from two classes c1 and c2')\n",
    "plt.xlabel('height [m]')\n",
    "plt.ylabel('number')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#\n",
    "# PLOT 2 both classes in 1D\n",
    "plt.plot(x2_h,np.zeros([N_h,1]),'co', label=\"hobbit\")\n",
    "plt.plot(x2_e,np.zeros([N_e,1]),'mo', label=\"elf\")\n",
    "plt.title('Training samples from two classes c1 and c2')\n",
    "plt.legend()\n",
    "plt.xlabel('weight [kg]')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(x2_h, bins = 10, alpha=0.5, color='cyan')\n",
    "plt.hist(x2_e, bins = 10, alpha=0.5, color='magenta')\n",
    "plt.title('Training samples from two classes c1 and c2')\n",
    "plt.xlabel('weight [kg]')\n",
    "plt.ylabel('number')\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "# PLOT 2 both classes in 2D\n",
    "plt.plot(x1_h,x2_h,'co', label=\"hobbit\")\n",
    "plt.plot(x1_e,x2_e,'mo', label=\"elf\")\n",
    "plt.title('Training samples from two classes c1 and c2')\n",
    "plt.legend()\n",
    "plt.xlabel('height [m]')\n",
    "plt.ylabel('weight [kg]')\n",
    "#plt.axis([0.5,2.5,-1.1,+1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw a number of test samples from the same 2D distributions and classify them (assuming they are separable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's make test data and test\n",
    "N_h_test = 10\n",
    "x1_h_test = np.random.normal(1.1,0.3,N_h_test)\n",
    "x2_h_test_noise =  np.random.normal(0,8,N_h_test)\n",
    "x2_h_test = a_h*x1_h_test+b_h+x2_h_test_noise\n",
    "c_h_test = np.zeros(N_h_test)\n",
    "c_h_test[:] = 1\n",
    "\n",
    "N_e_test = 10\n",
    "x1_e_test = np.random.normal(1.9,0.4,N_e_test)\n",
    "x2_e_test_noise =  np.random.normal(0,8,N_e_test)\n",
    "x2_e_test = a_e*x1_e_test+b_e+x2_e_test_noise\n",
    "c_e_test = np.zeros(N_e_test)\n",
    "c_e_test[:] = 2\n",
    "\n",
    "plt.plot(x1_h,x2_h,'co', label=\"hobbit\")\n",
    "plt.plot(x1_e,x2_e,'mo', label=\"elf\")\n",
    "plt.plot(x1_h_test,x2_h_test,'ko', label=\"test (hob)\")\n",
    "plt.plot(x1_e_test,x2_e_test,'kd', label=\"test (elf)\")\n",
    "plt.title('Training and test samples from two classes c1 ja c2')\n",
    "plt.legend()\n",
    "plt.xlabel('height [m]')\n",
    "plt.ylabel('weight [kg]')\n",
    "#plt.axis([0.5,2.5,-1.1,+1.1])\n",
    "plt.show()\n",
    "\n",
    "x1_test = np.concatenate((x1_h_test,x1_e_test))\n",
    "x2_test = np.concatenate((x2_h_test,x2_e_test))\n",
    "c_test = np.concatenate((c_h_test,c_e_test))\n",
    "c_test_hat = np.zeros(c_test.shape)\n",
    "\n",
    "priori_h = N_h/(N_h+N_e)\n",
    "mu1_h = np.mean(x1_h)\n",
    "mu2_h = np.mean(x2_h)\n",
    "sigma2_1_h = np.var(x1_h)\n",
    "sigma2_2_h = np.var(x2_h)\n",
    "p_h_1 = 1/np.sqrt(2*np.pi*sigma2_1_h)*np.exp(-1/(2*sigma2_1_h)*(x1_test-mu1_h)**2)\n",
    "P_h_1 = priori_h*p_h_1\n",
    "p_h_2 = 1/np.sqrt(2*np.pi*sigma2_2_h)*np.exp(-1/(2*sigma2_2_h)*(x2_test-mu2_h)**2)\n",
    "P_h_2 = priori_h*p_h_2\n",
    "P_h = priori_h*p_h_1*p_h_2 # Naive rule for hobbits\n",
    "\n",
    "\n",
    "priori_e = N_e/(N_h+N_e)\n",
    "mu1_e = np.mean(x1_e)\n",
    "mu2_e = np.mean(x2_e)\n",
    "sigma2_1_e = np.var(x1_e)\n",
    "sigma2_2_e = np.var(x2_e)\n",
    "p_e_1 = 1/np.sqrt(2*np.pi*sigma2_1_e)*np.exp(-1/(2*sigma2_1_e)*(x1_test-mu1_e)**2)\n",
    "P_e_1 = priori_e*p_e_1\n",
    "p_e_2 = 1/np.sqrt(2*np.pi*sigma2_2_e)*np.exp(-1/(2*sigma2_2_e)*(x2_test-mu2_e)**2)\n",
    "P_e_2 = priori_e*p_e_2\n",
    "P_e = priori_e*p_e_1*p_e_2 # Naive rule for elves\n",
    "\n",
    "c_test_hat[np.argwhere(np.greater(P_h_1,P_e_1) == True)] = 1\n",
    "c_test_hat[np.argwhere(np.greater(P_h_1,P_e_1) == False)] = 2\n",
    "corr = np.count_nonzero(np.equal(c_test,c_test_hat))\n",
    "success_rate = corr/(N_h_test+N_e_test)\n",
    "print(f'Success rate using height: {success_rate:.2f}')\n",
    "\n",
    "c_test_hat[np.argwhere(np.greater(P_h_2,P_e_2) == True)] = 1\n",
    "c_test_hat[np.argwhere(np.greater(P_h_2,P_e_2) == False)] = 2\n",
    "corr = np.count_nonzero(np.equal(c_test,c_test_hat))\n",
    "success_rate = corr/(N_h_test+N_e_test)\n",
    "print(f'Success rate using weight: {success_rate:.2f}')\n",
    "\n",
    "c_test_hat[np.argwhere(np.greater(P_h,P_e) == True)] = 1\n",
    "c_test_hat[np.argwhere(np.greater(P_h,P_e) == False)] = 2\n",
    "corr = np.count_nonzero(np.equal(c_test,c_test_hat))\n",
    "success_rate = corr/(N_h_test+N_e_test)\n",
    "print(f'Success rate using both (assuming they\\'re separable): {success_rate:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about if we use the genuine 2D Gaussian distribution\n",
    "\\begin{equation}\n",
    "  \\mathcal{N}(\\vec{x}; \\vec{\\mu},\\mathbf{\\Sigma}) =\n",
    "  \\frac{1}{2\\pi|\\mathbf{\\Sigma}|^{\\frac{1}{2}}}e^{-\\frac{1}{2}(\\vec{x}-\\vec{\\mu})^T\\mathbf{\\Sigma}^{-1}(\\vec{x}-\\vec{\\mu})} \\enspace .\n",
    "  \\label{eq:2dgaussian}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "print('Mean and covariance for hobbits')\n",
    "X_h = np.concatenate(([x1_h],[x2_h]),axis=0)\n",
    "mu_h = np.mean(X_h,axis=1)\n",
    "print(mu_h)\n",
    "Sigma_h = np.cov(X_h)\n",
    "print(Sigma_h)\n",
    "priori_h = N_h/(N_h+N_e)\n",
    "\n",
    "\n",
    "print('Mean and covariance for elves')\n",
    "X_e = np.concatenate(([x1_e],[x2_e]),axis=0)\n",
    "mu_e = np.mean(X_e,axis=1)\n",
    "print(mu_e)\n",
    "Sigma_e = np.cov(X_e)\n",
    "print(Sigma_e)\n",
    "priori_e = N_e/(N_h+N_e)\n",
    "\n",
    "def gaussian2d(X,Y, mu, Sigma, priori):\n",
    "    Z = np.zeros(X.shape)\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Z[i,j] = priori*1/np.sqrt(2*np.pi)*1/np.sqrt(np.linalg.det(Sigma))*np.exp(-1/2*np.transpose([X[i,j]-mu[0], Y[i,j]-mu[1]]) @ inv(Sigma) @ [X[i,j]-mu[0], Y[i,j]-mu[1]])\n",
    "    return Z\n",
    "\n",
    "x_1 = np.linspace(0,3.0,50)\n",
    "x_2 = np.linspace(0,150.0,50)\n",
    "\n",
    "X, Y = np.meshgrid(x_1,x_2)\n",
    "Z_h = gaussian2d(X,Y, mu_h, Sigma_h, priori_h)\n",
    "Z_e = gaussian2d(X,Y, mu_e, Sigma_e, priori_e)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X, Y, Z_h, 50, cmap='Greens')\n",
    "ax.contour3D(X, Y, Z_e, 50, cmap='Reds')\n",
    "ax.set_xlabel('height [m]')\n",
    "ax.set_ylabel('weight [kg]')\n",
    "ax.set_zlabel('Probability density');\n",
    "#ax.legend()\n",
    "plt.show()\n",
    "\n",
    "P_h = np.zeros(x1_test.shape)\n",
    "P_e = np.zeros(x1_test.shape)\n",
    "for i in range(x1_test.shape[0]):\n",
    "    P_h[i] = priori_h*1/np.sqrt(2*np.pi)*1/np.sqrt(np.linalg.det(Sigma_h))*np.exp(-1/2*np.transpose([x1_test[i]-mu_h[0], x2_test[i]-mu_h[1]]) @ inv(Sigma_h) @ [x1_test[i]-mu_h[0], x2_test[i]-mu_h[1]])\n",
    "    P_e[i] = priori_e*1/np.sqrt(2*np.pi)*1/np.sqrt(np.linalg.det(Sigma_e))*np.exp(-1/2*np.transpose([x1_test[i]-mu_e[0], x2_test[i]-mu_e[1]]) @ inv(Sigma_e) @ [x1_test[i]-mu_e[0], x2_test[i]-mu_e[1]])\n",
    "\n",
    "#print(P_h)\n",
    "c_test_hat[np.argwhere(np.greater(P_h,P_e) == True)] = 1\n",
    "c_test_hat[np.argwhere(np.greater(P_h,P_e) == False)] = 2\n",
    "corr = np.count_nonzero(np.equal(c_test,c_test_hat))\n",
    "success_rate = corr/(N_h_test+N_e_test)\n",
    "print(f'Success rate with 2D Gaussian (non-separable): {success_rate:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D-dimensional Gaussian\n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathcal{N}(\\vec{x}; \\vec{\\mu},\\mathbf{\\Sigma}) =\n",
    "  \\frac{1}{(2\\pi)^{\\frac{D}{2}}|\\mathbf{\\Sigma}|^{\\frac{1}{2}}}e^{-\\frac{1}{2}(\\vec{x}-\\vec{\\mu})^T\\mathbf{\\Sigma}^{-1}(\\vec{x}-\\vec{\\mu})}\n",
    "  \\label{eq:Ddgaussian}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make separable (Naive Bayes classifier)\n",
    "\n",
    "We assume that all observation variables are independent from each other.\n",
    "\n",
    "\\begin{equation}\n",
    "  \\begin{split}\n",
    "    \\mathbf{\\Sigma}_{3\\times 3} &=\n",
    "    \\left[ \\begin{array}{c c c } \\sigma_1^2 & \\sigma_1\\sigma_2 & \\sigma_1\\sigma_3\\\\\n",
    "        \\sigma_2\\sigma_1 & \\sigma_2^2 & \\sigma_2\\sigma_2\\\\\n",
    "        \\sigma_3\\sigma_1 & \\sigma_3\\sigma_2 & \\sigma_3^2 \\end{array} \\right]\\\\         \n",
    "    \\mathbf{\\Sigma}_{3\\times 3}^{naive} &=\n",
    "    \\left[ \\begin{array}{c c c } \\sigma_1^2 & 0 & 0 \\\\\n",
    "        0 & \\sigma_2^2 & 0\\\\\n",
    "        0 & 0 & \\sigma_3^2 \\end{array} \\right]\\\\     \n",
    "\\end{split} \\enspace .\n",
    "  \\label{eq:sigma_naive}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian mixture model\n",
    "\n",
    "\\begin{equation}\n",
    "  P(\\vec{x}; \\left\\{ \\alpha_c, \\vec{\\mu}_c, \\mathbf{\\Sigma}_c\\right\\}_{c=1,\\ldots,C}) = \\sum_{c=1}^C \\alpha_c \\mathcal{N}(\\vec{x}; \\vec{\\mu}_c,\\mathbf{\\Sigma}_c) \\enspace .\n",
    "\\end{equation}\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "  \\sum_{c=1}^C \\alpha_c = 1 \\enspace .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "#\n",
    "# Let's make 2D correlated data of hobbits and elfs\n",
    "\n",
    "# Hobits\n",
    "np.random.seed(42) # to always get the same points\n",
    "N_h = 80 \n",
    "x1_h = np.random.normal(1.1,0.3,N_h)\n",
    "a_h = 50.0\n",
    "b_h = 20.0\n",
    "x2_h_noise =  np.random.normal(0,8,N_h)\n",
    "x2_h = a_h*x1_h+b_h+x2_h_noise\n",
    "\n",
    "# Elves\n",
    "N_e = 20 \n",
    "x1_e = np.random.normal(1.9,0.4,N_e)\n",
    "a_e = 30.0\n",
    "b_e = 30.0\n",
    "x2_e_noise =  np.random.normal(0,8,N_e)\n",
    "x2_e = a_e*x1_e+b_e+x2_e_noise\n",
    "\n",
    "# Dwarfs\n",
    "N_d = 60 \n",
    "x1_d = np.random.normal(1.0,0.1,N_d)\n",
    "a_d = 50.0\n",
    "b_d = 40.0\n",
    "x2_d_noise =  np.random.normal(0,6,N_d)\n",
    "x2_d = a_d*x1_d+b_d+x2_d_noise\n",
    "\n",
    "print('Mean and covariance for hobits')\n",
    "X_h = np.concatenate(([x1_h],[x2_h]),axis=0)\n",
    "mu_h = np.mean(X_h,axis=1)\n",
    "print(mu_h)\n",
    "Sigma_h = np.cov(X_h)\n",
    "print(Sigma_h)\n",
    "priori_h = N_h/(N_h+N_e+N_d)\n",
    "\n",
    "\n",
    "print('Mean and covariance for elfs')\n",
    "X_e = np.concatenate(([x1_e],[x2_e]),axis=0)\n",
    "mu_e = np.mean(X_e,axis=1)\n",
    "print(mu_e)\n",
    "Sigma_e = np.cov(X_e)\n",
    "print(Sigma_e)\n",
    "priori_e = N_e/(N_h+N_e+N_d)\n",
    "\n",
    "print('Mean and covariance for dwarfs')\n",
    "X_d = np.concatenate(([x1_d],[x2_d]),axis=0)\n",
    "mu_d = np.mean(X_d,axis=1)\n",
    "print(mu_d)\n",
    "Sigma_d = np.cov(X_d)\n",
    "print(Sigma_d)\n",
    "priori_d = N_d/(N_h+N_e+N_d)\n",
    "\n",
    "print('Mean and covariance for dwarfs and elves combined')\n",
    "X_ed = np.concatenate((X_d, X_e),axis=1)\n",
    "mu_ed = np.mean(X_ed,axis=1)\n",
    "print(mu_ed)\n",
    "Sigma_ed = np.cov(X_ed)\n",
    "print(Sigma_ed)\n",
    "priori_ed = (N_d+N_e)/(N_h+N_e+N_d)\n",
    "\n",
    "def gaussian2d(X,Y, mu, Sigma, priori):\n",
    "    Z = np.zeros(X.shape)\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Z[i,j] = priori*1/np.sqrt(2*np.pi)*1/np.sqrt(np.linalg.det(Sigma))*np.exp(-1/2*np.transpose([X[i,j]-mu[0], Y[i,j]-mu[1]]) @ inv(Sigma) @ [X[i,j]-mu[0], Y[i,j]-mu[1]])\n",
    "    return Z\n",
    "\n",
    "x_1 = np.linspace(0,3.0,70)\n",
    "x_2 = np.linspace(0,150.0,70)\n",
    "\n",
    "X, Y = np.meshgrid(x_1,x_2)\n",
    "Z_h = gaussian2d(X,Y, mu_h, Sigma_h, priori_h)\n",
    "Z_e = gaussian2d(X,Y, mu_e, Sigma_e, priori_e)\n",
    "Z_d = gaussian2d(X,Y, mu_d, Sigma_d, priori_d)\n",
    "Z_ed = gaussian2d(X,Y, mu_ed, Sigma_ed, priori_ed)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X, Y, Z_h, 50, cmap='Greens')\n",
    "ax.contour3D(X, Y, Z_e, 50, cmap='Reds')\n",
    "ax.contour3D(X, Y, Z_d, 50, cmap='Blues')\n",
    "ax.set_xlabel('height [m]')\n",
    "ax.set_ylabel('weight [kg]')\n",
    "ax.set_zlabel('Probability density');\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X, Y, Z_h, 50, cmap='Greens')\n",
    "ax.contour3D(X, Y, Z_ed, 50, cmap='Greys')\n",
    "ax.set_xlabel('height [m]')\n",
    "ax.set_ylabel('weight [kg]')\n",
    "ax.set_zlabel('Probability density');\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "# Run Gaussian mixture model\n",
    "gm = GaussianMixture(n_components=2, random_state=0).fit(X_ed.T)\n",
    "mu_ed1 = gm.means_[0]\n",
    "mu_ed2 = gm.means_[1]\n",
    "Sigma_ed1 = gm.covariances_[0]\n",
    "Sigma_ed2 = gm.covariances_[1]\n",
    "priori_ed1 = gm.weights_[0]/(gm.weights_[0]+gm.weights_[1])*priori_ed\n",
    "priori_ed2 = gm.weights_[1]/(gm.weights_[0]+gm.weights_[1])*priori_ed\n",
    "Z_ed_1 = gaussian2d(X,Y, mu_ed1, Sigma_ed1, priori_ed1)\n",
    "Z_ed_2 = gaussian2d(X,Y, mu_ed2, Sigma_ed2, priori_ed2)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X, Y, Z_h, 50, cmap='Greens')\n",
    "ax.contour3D(X, Y, Z_ed_1, 50, cmap='Greys')\n",
    "ax.contour3D(X, Y, Z_ed_2, 50, cmap='Greys')\n",
    "ax.set_xlabel('height [m]')\n",
    "ax.set_ylabel('weight [kg]')\n",
    "ax.set_zlabel('Probability density');\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "# Let's make test data and test\n",
    "\n",
    "N_h_test = 10\n",
    "x1_h_test = np.random.normal(1.1,0.3,N_h_test)\n",
    "x2_h_test_noise =  np.random.normal(0,8,N_h_test)\n",
    "x2_h_test = a_h*x1_h_test+b_h+x2_h_test_noise\n",
    "c_h_test = np.zeros(N_h_test)\n",
    "c_h_test[:] = 1\n",
    "\n",
    "N_e_test = 10\n",
    "x1_e_test = np.random.normal(1.9,0.4,N_e_test)\n",
    "x2_e_test_noise =  np.random.normal(0,8,N_e_test)\n",
    "x2_e_test = a_e*x1_e_test+b_e+x2_e_test_noise\n",
    "c_e_test = np.zeros(N_e_test)\n",
    "c_e_test[:] = 2\n",
    "\n",
    "x1_test = np.concatenate((x1_h_test,x1_e_test))\n",
    "x2_test = np.concatenate((x2_h_test,x2_e_test))\n",
    "c_test = np.concatenate((c_h_test,c_e_test))\n",
    "\n",
    "P_h = np.zeros(x1_test.shape)\n",
    "P_e = np.zeros(x1_test.shape)\n",
    "P_ed = np.zeros(x1_test.shape)\n",
    "P_ed2 = np.zeros(x1_test.shape)\n",
    "for i in range(x1_test.shape[0]):\n",
    "    P_h[i] = priori_h*1/np.sqrt(2*np.pi)*1/np.sqrt(np.linalg.det(Sigma_h))*np.exp(-1/2*np.transpose([x1_test[i]-mu_h[0], x2_test[i]-mu_h[1]]) @ inv(Sigma_h) @ [x1_test[i]-mu_h[0], x2_test[i]-mu_h[1]])\n",
    "    P_e[i] = priori_e*1/np.sqrt(2*np.pi)*1/np.sqrt(np.linalg.det(Sigma_e))*np.exp(-1/2*np.transpose([x1_test[i]-mu_e[0], x2_test[i]-mu_e[1]]) @ inv(Sigma_e) @ [x1_test[i]-mu_e[0], x2_test[i]-mu_e[1]])\n",
    "    P_ed[i] = priori_ed*1/np.sqrt(2*np.pi)*1/np.sqrt(np.linalg.det(Sigma_ed))*np.exp(-1/2*np.transpose([x1_test[i]-mu_ed[0], x2_test[i]-mu_ed[1]]) @ inv(Sigma_ed) @ [x1_test[i]-mu_ed[0], x2_test[i]-mu_ed[1]])\n",
    "    P_ed2[i] = priori_ed1*1/np.sqrt(2*np.pi)*1/np.sqrt(np.linalg.det(Sigma_ed1))*np.exp(-1/2*np.transpose([x1_test[i]-mu_ed1[0], x2_test[i]-mu_ed1[1]]) @ inv(Sigma_ed1) @ [x1_test[i]-mu_ed1[0], x2_test[i]-mu_ed1[1]])+priori_ed2*1/np.sqrt(2*np.pi)*1/np.sqrt(np.linalg.det(Sigma_ed2))*np.exp(-1/2*np.transpose([x1_test[i]-mu_ed2[0], x2_test[i]-mu_ed2[1]]) @ inv(Sigma_ed2) @ [x1_test[i]-mu_ed2[0], x2_test[i]-mu_ed2[1]])\n",
    "\n",
    "\n",
    "#print(P_h)\n",
    "#print(P_e)\n",
    "#print(P_ed)\n",
    "#print(np.greater(P_h,P_ed))\n",
    "c_test_hat = np.zeros(c_test.shape)\n",
    "c_test_hat[np.argwhere(np.greater(P_h,P_e) == True)] = 1\n",
    "c_test_hat[np.argwhere(np.greater(P_h,P_e) == False)] = 2\n",
    "corr = np.count_nonzero(np.equal(c_test,c_test_hat))\n",
    "success_rate = corr/(N_h_test+N_e_test)\n",
    "print(f'Success rate using hobbits and elves: {success_rate:.2f}')\n",
    "\n",
    "c_test_hat = np.zeros(c_test.shape)\n",
    "c_test_hat[np.argwhere(np.greater(P_h,P_ed) == True)] = 1\n",
    "c_test_hat[np.argwhere(np.greater(P_h,P_ed) == False)] = 2\n",
    "corr = np.count_nonzero(np.equal(c_test,c_test_hat))\n",
    "success_rate = corr/(N_h_test+N_e_test)\n",
    "print(f'Success rate using hobbits and mixture of dwarfs and elves: {success_rate:.2f}')\n",
    "\n",
    "c_test_hat = np.zeros(c_test.shape)\n",
    "c_test_hat[np.argwhere(np.greater(P_h,P_ed2) == True)] = 1\n",
    "c_test_hat[np.argwhere(np.greater(P_h,P_ed2) == False)] = 2\n",
    "corr = np.count_nonzero(np.equal(c_test,c_test_hat))\n",
    "success_rate = corr/(N_h_test+N_e_test)\n",
    "print(f'Success rate using hobbits and mixture but with GMM: {success_rate:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-parametric probability density models\n",
    "\n",
    "For $N$ samples $x_n$ we define the probability at point $x$ as a Gaussian centered at $x$ ($\\mu = x$) - this produces the probability value at location $x$ which is made by weighing each training sample according to a Gaussian window:\n",
    "\n",
    "\\begin{equation}\n",
    "P(x) = \\frac{1}{N} \\sum_{n=1}^N \\frac{1}{h} k\\left(\\frac{x-x_n}{h}\\right) = \\sum_{n=1}^N \\mathcal{N}\\left(x_n; x,h\\right) \\enspace\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play the only parameter of this model that is the window width $h$ that corresponds to the Gaussian variance $\\sigma^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# 1. Generate and plot random points for training\n",
    "np.random.seed(66) # to always get the same points\n",
    "N_h = 1000\n",
    "N_e = 200\n",
    "mu_h_gt = 1.1\n",
    "mu_e_gt = 1.9\n",
    "sigma_h_gt = 0.3\n",
    "sigma_e_gt = 0.4\n",
    "x_h = np.random.normal(mu_h_gt,sigma_h_gt,N_h)\n",
    "x_e = np.random.normal(mu_e_gt,sigma_e_gt,N_e)\n",
    "plt.plot(x_h,np.zeros([N_h,1]),'co', label=\"hobit\")\n",
    "plt.plot(x_e,np.zeros([N_e,1]),'mo', label=\"elf\")\n",
    "plt.title(f'{N_h} hobit height')\n",
    "plt.legend()\n",
    "plt.xlabel('height [m]')\n",
    "plt.axis([0.0,3.0,-1.1,+1.1])\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "#\n",
    "mu_h_est = np.mean(x_h)\n",
    "mu_e_est = np.mean(x_e)\n",
    "sigma_h_est = np.std(x_h)\n",
    "sigma_e_est = np.std(x_e)\n",
    "print(f'Avg height hobits {mu_h_est:0.2f} (GT: {mu_h_gt:0.2f})')\n",
    "print(f'Avg height elves {mu_e_est:0.2f} (GT: {mu_e_gt:0.2f})')\n",
    "print(f'Height st. deviation hobits {sigma_h_est:0.3f} (GT: {sigma_h_gt:0.3f})')\n",
    "print(f'Height st. deviation elves {sigma_e_est:0.3f} (GT: {sigma_e_gt:0.3f})')\n",
    "\n",
    "def gaussian(x, mu, sigma, priori):\n",
    "    z = np.zeros(x.shape)\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = priori*1/np.sqrt(2*np.pi)*1/sigma*np.exp((-1/2*(x[i]-mu)**2)/(2*sigma**2))\n",
    "    return z\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "[x, step_size] = np.linspace(0,3.0,70,retstep=True)\n",
    "lhood_h_est = gaussian(x, mu_h_est, sigma_h_est, 1)\n",
    "lhood_e_est = gaussian(x, mu_e_est, sigma_e_est, 1)\n",
    "lhood_h_gt = gaussian(x, mu_h_gt, sigma_h_gt, 1)\n",
    "lhood_e_gt = gaussian(x, mu_e_gt, sigma_e_gt, 1)\n",
    "plt.plot(x_h,np.zeros([N_h,1]),'co', label=\"hobit\")\n",
    "plt.plot(x_e,np.zeros([N_e,1]),'mo', label=\"elf\")\n",
    "plt.plot(x,lhood_h_gt,'c-', label=\"hobit (GT)\")\n",
    "plt.plot(x,lhood_e_gt,'m-', label=\"elf (GT)\")\n",
    "plt.plot(x,lhood_h_est,'c--', label=\"hobit (est)\")\n",
    "plt.plot(x,lhood_e_est,'m--', label=\"elf (est)\")\n",
    "plt.legend()\n",
    "plt.xlabel('pituus [m]')\n",
    "#plt.axis([0.0,3.0,-1.1,+5])\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "#\n",
    "kern_width = 0.2\n",
    "[x, step_size] = np.linspace(0,3.0,70,retstep=True)\n",
    "plt.plot(x_h,np.zeros([N_h,1]),'co', label=\"hobit\")\n",
    "plt.plot(x_e,np.zeros([N_e,1]),'mo', label=\"elf\")\n",
    "[x_kern, step_size_kern] = np.linspace(0,3.0,11,retstep=True)\n",
    "x_kern_plot = np.linspace(-1.0,+4.0,201)\n",
    "for foo_ind, foo_val in enumerate(x_kern):\n",
    "    foo_kern = gaussian(x_kern_plot, foo_val, kern_width, 1)\n",
    "    plt.plot(x_kern_plot, foo_kern,'y--',label='kernel')\n",
    "    break\n",
    "plt.legend()\n",
    "plt.xlabel('height [m]')\n",
    "#plt.axis([0.0,3.0,-1.1,+5])\n",
    "plt.show()\n",
    "\n",
    "# Output value is Gaussian kernel multiplied by all positive samples\n",
    "lhood_h_est_kern = np.zeros(len(x))\n",
    "for xind, xval in enumerate(x):\n",
    "    lhood_h_est_kern[xind] = sum(gaussian(x_h,xval,kern_width,1))/len(x_h)\n",
    "    #lhood_h_est_kern[xind] = sum(stats.norm.pdf(x_h, xval, kernel_width))\n",
    "\n",
    "plt.plot(x_h,np.zeros([N_h,1]),'co', label=\"hobit\")\n",
    "plt.plot(x_e,np.zeros([N_e,1]),'mo', label=\"elf\")\n",
    "plt.plot(x,lhood_h_gt,'c-', label=\"hobit (GT)\")\n",
    "plt.plot(x,lhood_h_est_kern,'c--', label=\"hobit (est)\")\n",
    "#plt.plot(x,lhood_e_est,'m--', label=\"haltija (est)\")\n",
    "plt.legend()\n",
    "plt.xlabel('height [m]')\n",
    "#plt.axis([0.0,3.0,-1.1,+5])\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "#\n",
    "kern_width = 0.02\n",
    "[x, step_size] = np.linspace(0,3.0,70,retstep=True)\n",
    "plt.plot(x_h,np.zeros([N_h,1]),'co', label=\"hobit\")\n",
    "plt.plot(x_e,np.zeros([N_e,1]),'mo', label=\"elf\")\n",
    "[x_kern, step_size_kern] = np.linspace(0,3.0,11,retstep=True)\n",
    "x_kern_plot = np.linspace(-1.0,+4.0,201)\n",
    "for foo_ind, foo_val in enumerate(x_kern):\n",
    "    foo_kern = gaussian(x_kern_plot, foo_val, kern_width, 1)\n",
    "    plt.plot(x_kern_plot, foo_kern,'y--',label='kerneli')\n",
    "    break\n",
    "plt.legend()\n",
    "plt.xlabel('height [m]')\n",
    "#plt.axis([0.0,3.0,-1.1,+5])\n",
    "plt.show()\n",
    "\n",
    "# Output value is Gaussian kernel multiplied by all positive samples\n",
    "lhood_h_est_kern = np.zeros(len(x))\n",
    "for xind, xval in enumerate(x):\n",
    "    lhood_h_est_kern[xind] = sum(gaussian(x_h,xval,kern_width,1))/len(x_h)\n",
    "    #lhood_h_est_kern[xind] = sum(stats.norm.pdf(x_h, xval, kernel_width))\n",
    "\n",
    "plt.plot(x_h,np.zeros([N_h,1]),'co', label=\"hobit\")\n",
    "plt.plot(x_e,np.zeros([N_e,1]),'mo', label=\"elf\")\n",
    "plt.plot(x,lhood_h_gt,'c-', label=\"hobit (GT)\")\n",
    "plt.plot(x,lhood_h_est_kern,'c--', label=\"hobit (est)\")\n",
    "#plt.plot(x,lhood_e_est,'m--', label=\"haltija (est)\")\n",
    "plt.legend()\n",
    "plt.xlabel('height [m]')\n",
    "#plt.axis([0.0,3.0,-1.1,+5])\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "#\n",
    "kern_width = 0.8\n",
    "[x, step_size] = np.linspace(0,3.0,70,retstep=True)\n",
    "plt.plot(x_h,np.zeros([N_h,1]),'co', label=\"hobit\")\n",
    "plt.plot(x_e,np.zeros([N_e,1]),'mo', label=\"elf\")\n",
    "[x_kern, step_size_kern] = np.linspace(0,3.0,11,retstep=True)\n",
    "x_kern_plot = np.linspace(-1.0,+4.0,201)\n",
    "for foo_ind, foo_val in enumerate(x_kern):\n",
    "    foo_kern = gaussian(x_kern_plot, foo_val, kern_width, 1)\n",
    "    plt.plot(x_kern_plot, foo_kern,'y--',label='kernel')\n",
    "    break\n",
    "plt.legend()\n",
    "plt.xlabel('height [m]')\n",
    "#plt.axis([0.0,3.0,-1.1,+5])\n",
    "plt.show()\n",
    "\n",
    "# Output value is Gaussian kernel multiplied by all positive samples\n",
    "lhood_h_est_kern = np.zeros(len(x))\n",
    "for xind, xval in enumerate(x):\n",
    "    lhood_h_est_kern[xind] = sum(gaussian(x_h,xval,kern_width,1))/len(x_h)\n",
    "    #lhood_h_est_kern[xind] = sum(stats.norm.pdf(x_h, xval, kernel_width))\n",
    "\n",
    "plt.plot(x_h,np.zeros([N_h,1]),'co', label=\"hobit\")\n",
    "plt.plot(x_e,np.zeros([N_e,1]),'mo', label=\"elf\")\n",
    "plt.plot(x,lhood_h_gt,'c-', label=\"hobit (GT)\")\n",
    "plt.plot(x,lhood_h_est_kern,'c--', label=\"hobit (est)\")\n",
    "#plt.plot(x,lhood_e_est,'m--', label=\"haltija (est)\")\n",
    "plt.legend()\n",
    "plt.xlabel('height [m]')\n",
    "#plt.axis([0.0,3.0,-1.1,+5])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "C.M. Bishop (2006): Pattern Recognition and Machine Learning, Chapter 1-2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
